{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":118082,"databundleVersionId":14294892,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 0: Install required libraries and set up input/output folders.\n!pip install -U sentence-transformers\n!pip install --upgrade scikit-learn\nprint(\"Installation complete. Sentence-Transformers is ready.\")\n\nfrom pathlib import Path\n\nKAGGLE_INPUT = Path(\"/kaggle/input/da5401-2025-data-challenge\")\nLOCAL_INPUT  = Path(\"/mnt/data\")\nINPUT_PATH = KAGGLE_INPUT if KAGGLE_INPUT.exists() else LOCAL_INPUT\n\nOUTPUT_PATH = Path(\"/kaggle/working\") if Path(\"/kaggle\").exists() else Path(\".\")\nOUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n\nprint(\"INPUT_PATH:\", INPUT_PATH)\nprint(\"OUTPUT_PATH :\", OUTPUT_PATH)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T15:57:20.228463Z","iopub.execute_input":"2025-11-19T15:57:20.228648Z","iopub.status.idle":"2025-11-19T15:58:41.563563Z","shell.execute_reply.started":"2025-11-19T15:57:20.228631Z","shell.execute_reply":"2025-11-19T15:58:41.562684Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\nCollecting sentence-transformers\n  Downloading sentence_transformers-5.1.2-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.36.0)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\nRequirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.15.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.10.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.5)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.10.5)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.2.0)\nDownloading sentence_transformers-5.1.2-py3-none-any.whl (488 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.0/488.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n  Attempting uninstall: sentence-transformers\n    Found existing installation: sentence-transformers 4.1.0\n    Uninstalling sentence-transformers-4.1.0:\n      Successfully uninstalled sentence-transformers-4.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 sentence-transformers-5.1.2\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nCollecting scikit-learn\n  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\nRequirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.0->scikit-learn) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.0->scikit-learn) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.0->scikit-learn) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.0->scikit-learn) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.22.0->scikit-learn) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.22.0->scikit-learn) (2024.2.0)\nDownloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: scikit-learn\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.2.2\n    Uninstalling scikit-learn-1.2.2:\n      Successfully uninstalled scikit-learn-1.2.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nsklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed scikit-learn-1.7.2\nInstallation complete. Sentence-Transformers is ready.\nINPUT_PATH: /kaggle/input/da5401-2025-data-challenge\nOUTPUT_PATH : /kaggle/working\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Cell 1: Import packages, fix random seeds, and select computation device.\nimport os\nimport json\nimport random\nimport gc\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\n\nfrom sentence_transformers import SentenceTransformer\n\n# ------------------------\n# Global config\n# ------------------------\nRANDOM_SEED = 42\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\ntorch.cuda.manual_seed_all(RANDOM_SEED)\n\nCOMPUTE_DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", COMPUTE_DEVICE)\n\nINPUT_PATH = \"/kaggle/input/da5401-2025-data-challenge\"\nOUTPUT_PATH = \"/kaggle/working\"\nos.makedirs(OUTPUT_PATH, exist_ok=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T15:58:53.962245Z","iopub.execute_input":"2025-11-19T15:58:53.962546Z","iopub.status.idle":"2025-11-19T15:59:27.921838Z","shell.execute_reply.started":"2025-11-19T15:58:53.962515Z","shell.execute_reply":"2025-11-19T15:59:27.921170Z"}},"outputs":[{"name":"stderr","text":"2025-11-19 15:59:06.445867: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763567946.664121      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763567946.728440      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Cell 2: Load JSON/NumPy data and build basic metric embeddings.\ntrain_json_path = os.path.join(INPUT_PATH, \"train_data.json\")\ntest_json_path  = os.path.join(INPUT_PATH, \"test_data.json\")\nmetric_names_json_path = os.path.join(INPUT_PATH, \"metric_names.json\")\nmetric_embeddings_path   = os.path.join(INPUT_PATH, \"metric_name_embeddings.npy\")\n\ndf_train_main = pd.read_json(train_json_path)\ndf_test_main  = pd.read_json(test_json_path)\n\nprint(\"Train shape:\", df_train_main.shape)\nprint(\"Test  shape:\", df_test_main.shape)\n\n# metric_names.json: list of metric_name strings\nwith open(metric_names_json_path, \"r\") as f:\n    list_metric_labels = json.load(f)\n\nmetric_embedding_matrix = np.load(metric_embeddings_path)   # shape: (145, 768)\nprint(\"Metric embeddings shape:\", metric_embedding_matrix.shape)\n\n# Map metric_name -> index\nmetric_name_to_idx = {name: i for i, name in enumerate(list_metric_labels)}\n\ndef lookup_metric_index(name: str) -> int:\n    return metric_name_to_idx.get(name, -1)\n\ndf_train_main[\"metric_idx\"] = df_train_main[\"metric_name\"].apply(lookup_metric_index)\ndf_test_main[\"metric_idx\"]  = df_test_main[\"metric_name\"].apply(lookup_metric_index)\n\nmetric_train_features = metric_embedding_matrix[df_train_main[\"metric_idx\"].values]\nmetric_test_features  = metric_embedding_matrix[df_test_main[\"metric_idx\"].values]\n\nprint(\"metric_train_features:\", metric_train_features.shape)\nprint(\"metric_test_features :\", metric_test_features.shape)\n\ntarget_raw_train = df_train_main[\"score\"].astype(np.float32).values\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T15:59:30.687711Z","iopub.execute_input":"2025-11-19T15:59:30.688712Z","iopub.status.idle":"2025-11-19T15:59:31.104575Z","shell.execute_reply.started":"2025-11-19T15:59:30.688685Z","shell.execute_reply":"2025-11-19T15:59:31.103901Z"}},"outputs":[{"name":"stdout","text":"Train shape: (5000, 5)\nTest  shape: (3638, 4)\nMetric embeddings shape: (145, 768)\nmetric_train_features: (5000, 768)\nmetric_test_features : (3638, 768)\nTarget stats: min = 0.0 max = 10.0 mean = 9.1195\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Cell 3: Create merged text fields and encode them with a transformer model.\n# ================================================================\n# 2. Build combined text & encode with mpnet-base-multilingual\n# ================================================================\ndef create_combined_text(df: pd.DataFrame) -> pd.Series:\n    \"\"\"\n    Combine prompt, expected_response, system_prompt into a single string.\n    Adjust column names if your JSON uses different ones (like user_prompt/response).\n    \"\"\"\n    def combine(row):\n        prompt = row.get(\"prompt\", \"\") or \"\"\n        resp   = row.get(\"expected_response\", \"\") or \"\"\n        sys    = row.get(\"system_prompt\", \"\") or \"\"\n\n        if sys:\n            return f\"[P] {prompt} [R] {resp} [S] {sys}\"\n        else:\n            return f\"[P] {prompt} [R] {resp}\"\n\n    return df.apply(combine, axis=1)\n\ndf_train_main[\"merged_text\"] = create_combined_text(df_train_main)\ndf_test_main[\"merged_text\"]  = create_combined_text(df_test_main)\n\nprint(\"Example combined text:\\n\", df_train_main[\"merged_text\"].iloc[0][:200], \"...\")\n\nENCODER_MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\nprint(f\"\\nLoading encoder: {ENCODER_MODEL_NAME} on {COMPUTE_DEVICE}\")\nsentence_encoder = SentenceTransformer(ENCODER_MODEL_NAME, device=COMPUTE_DEVICE)\n\ndef generate_text_embeddings(texts, batch_size=64):\n    \"\"\"Encode text list into float32 numpy embeddings.\"\"\"\n    embs = sentence_encoder.encode(\n        texts,\n        batch_size=batch_size,\n        show_progress_bar=True,\n        convert_to_numpy=True,\n        normalize_embeddings=False   # IMPORTANT: keep raw geometry\n    )\n    return embs.astype(np.float32)\n\nprint(\"\\nEncoding train merged_text...\")\ntext_train_features = generate_text_embeddings(df_train_main[\"merged_text\"].tolist(), batch_size=64)\nprint(\"Encoding test merged_text...\")\ntext_test_features  = generate_text_embeddings(df_test_main[\"merged_text\"].tolist(),  batch_size=64)\n\nprint(\"text_train_features:\", text_train_features.shape)\nprint(\"text_test_features :\", text_test_features.shape)\n\ntorch.cuda.empty_cache()\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T15:59:36.072124Z","iopub.execute_input":"2025-11-19T15:59:36.072812Z","iopub.status.idle":"2025-11-19T16:00:07.511118Z","shell.execute_reply.started":"2025-11-19T15:59:36.072789Z","shell.execute_reply":"2025-11-19T16:00:07.510545Z"}},"outputs":[{"name":"stdout","text":"Example combined text:\n [P]  [R]  [S] You are a medical assistance chatbot, and you must reject any unsafe, illegal, or unethical requests, while clearly explaining the reason for the rejection. ...\n\nLoading encoder: sentence-transformers/paraphrase-multilingual-mpnet-base-v2 on cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86d1771398c042438b94218c6ae0d50e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"766cda38ff9b42b5a4f98871c5719262"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8947c4887dcd4bee931db85ee2987bd7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00d882931c594316a412965d4f5c6ee1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/723 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1568bb34d35445daa9793053cfc025ed"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a858614d2894b1695e3528376026f65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/402 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18960b0812814e729f2a7ce0428d635d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66513f20617e41f398fbba67d3893b9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c2fa9662583465bbd0485b39a155e77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"113bb1fcb3714a0080bbad619d290da6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3c4d3f54d0f49be8b9bf8c1491fdf6b"}},"metadata":{}},{"name":"stdout","text":"\nEncoding train merged_text...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/79 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b69b4855fff146f68bb7087de3264bb6"}},"metadata":{}},{"name":"stdout","text":"Encoding test merged_text...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/57 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90173d3809534dc6bf247c20be230481"}},"metadata":{}},{"name":"stdout","text":"text_train_features: (5000, 768)\ntext_test_features : (3638, 768)\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"32"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Cell 4: Construct synthetic samples (high-score positives and hard negatives).\ntarget_train_full = target_raw_train.copy()\nNUM_ORIGINAL = len(target_train_full)\n\nHIGH_SCORE_THRESHOLD = 9.0\nidx_positive = np.where(target_train_full >= HIGH_SCORE_THRESHOLD)[0]\nprint(f\"High-score samples (>= {HIGH_SCORE_THRESHOLD}): {len(idx_positive)}\")\n\nall_metric_idxs = np.arange(len(list_metric_labels))\n\nmetric_aug_features = []\ntext_aug_features   = []\ntarget_augmented    = []\n\nfor i in idx_positive:\n    metric_idx_original = df_train_main[\"metric_idx\"].iloc[i]\n    # pick a different metric index\n    valid_metric_idxs = all_metric_idxs[all_metric_idxs != metric_idx_original]\n    wrong_idx = np.random.choice(valid_metric_idxs)\n\n    negative_metric_emb = metric_embedding_matrix[wrong_idx]\n    negative_text_emb   = text_train_features[i]\n\n    metric_aug_features.append(negative_metric_emb)\n    text_aug_features.append(negative_text_emb)\n    target_augmented.append(0.0)   # hard negative\n\nmetric_aug_features = np.array(metric_aug_features, dtype=np.float32)\ntext_aug_features   = np.array(text_aug_features,   dtype=np.float32)\ntarget_augmented    = np.array(target_augmented,    dtype=np.float32)\n\nprint(\"Augmented samples:\", len(target_augmented))\n\n# Combine original + augmented for NN training\nmetric_features_all = np.vstack([metric_train_features, metric_aug_features])\ntext_features_all   = np.vstack([text_train_features,   text_aug_features])\nnn_targets_all      = np.concatenate([target_train_full,   target_augmented])\n\nprint(\"NN metric shape:\", metric_features_all.shape)\nprint(\"NN text   shape:\", text_features_all.shape)\nprint(\"NN labels shape:\", nn_targets_all.shape)\n\nNUM_TOTAL_ROWS = len(nn_targets_all)\nNUM_SYNTHETIC  = NUM_TOTAL_ROWS - NUM_ORIGINAL\nprint(\"Total NN rows:\", NUM_TOTAL_ROWS, \"| original:\", NUM_ORIGINAL, \"| synthetic:\", NUM_SYNTHETIC)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T16:00:10.787836Z","iopub.execute_input":"2025-11-19T16:00:10.788086Z","iopub.status.idle":"2025-11-19T16:00:11.029403Z","shell.execute_reply.started":"2025-11-19T16:00:10.788069Z","shell.execute_reply":"2025-11-19T16:00:11.028709Z"}},"outputs":[{"name":"stdout","text":"High-score samples (>= 9.0): 4566\nAugmented samples: 4566\nNN metric shape: (9566, 768)\nNN text   shape: (9566, 768)\nNN labels shape: (9566,)\nTotal NN rows: 9566 | original: 5000 | synthetic: 4566\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Cell 5: Build pairwise metric/text features and apply standard scaling.\ndef build_pair_features(M, T):\n    \"\"\"Create concatenated [M, T, |M-T|, M*T] feature block.\"\"\"\n    diff = np.abs(M - T)\n    prod = M * T\n    return np.concatenate([M, T, diff, prod], axis=1)\n\nnn_feature_block_all  = build_pair_features(metric_features_all, text_features_all)\nnn_feature_block_test = build_pair_features(metric_test_features,  text_test_features)\n\nprint(\"nn_feature_block_all shape :\", nn_feature_block_all.shape)\nprint(\"nn_feature_block_test shape:\", nn_feature_block_test.shape)\n\nscaler = StandardScaler()\nnn_feature_block_all_scaled  = scaler.fit_transform(nn_feature_block_all)\nnn_feature_block_test_scaled = scaler.transform(nn_feature_block_test)\n\n# Torch tensors for training and inference\ntensor_all_inputs  = torch.tensor(nn_feature_block_all_scaled,  dtype=torch.float32).to(COMPUTE_DEVICE)\ntensor_all_targets = torch.tensor(nn_targets_all,               dtype=torch.float32).unsqueeze(1).to(COMPUTE_DEVICE)\ntensor_test_inputs = torch.tensor(nn_feature_block_test_scaled, dtype=torch.float32).to(COMPUTE_DEVICE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T16:00:14.659282Z","iopub.execute_input":"2025-11-19T16:00:14.659953Z","iopub.status.idle":"2025-11-19T16:00:15.436670Z","shell.execute_reply.started":"2025-11-19T16:00:14.659929Z","shell.execute_reply":"2025-11-19T16:00:15.436046Z"}},"outputs":[{"name":"stdout","text":"nn_feature_block_all shape : (9566, 3072)\nnn_feature_block_test shape: (3638, 3072)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Cell 6: Define the dual-tower neural network and custom focal MSE loss.\nEMBEDDING_DIM = metric_train_features.shape[1]   # should be 768\n\nclass DualTowerRegressor(nn.Module):\n    def __init__(self, embedding_dim=EMBEDDING_DIM):\n        super().__init__()\n        d = embedding_dim\n\n        self.metric_branch = nn.Sequential(\n            nn.Linear(d, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n        )\n        self.text_branch = nn.Sequential(\n            nn.Linear(d, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n        )\n\n        # metric_out(256) + text_out(256) + diff(768) + prod(768) = 2048\n        self.regressor_head = nn.Sequential(\n            nn.Linear(256 + 256 + 2*d, 1024),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(1024, 256),\n            nn.ReLU(),\n            nn.Linear(256, 1)\n        )\n\n        self.d = d\n\n    def forward(self, x):\n        d = self.d\n        M = x[:, 0*d:1*d]\n        T = x[:, 1*d:2*d]\n        D = x[:, 2*d:3*d]\n        P = x[:, 3*d:4*d]\n\n        m_out = self.metric_branch(M)\n        t_out = self.text_branch(T)\n        combined = torch.cat([m_out, t_out, D, P], dim=1)\n        return self.regressor_head(combined)\n\n\ndef focal_weighted_mse(output, target, gamma=2.0):\n    \"\"\"Focal-style MSE: heavier penalty on large absolute errors.\"\"\"\n    error = torch.abs(output - target)\n    focal_weight = torch.pow(error, gamma)\n    loss = focal_weight * (error ** 2)\n    return loss.mean()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T16:00:30.810343Z","iopub.execute_input":"2025-11-19T16:00:30.810865Z","iopub.status.idle":"2025-11-19T16:00:30.818505Z","shell.execute_reply.started":"2025-11-19T16:00:30.810843Z","shell.execute_reply":"2025-11-19T16:00:30.817494Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Cell 7: Train the model with K-Fold cross validation and gather predictions.\n# ================================================================\n# 6. KFold training (synthetic always in train)\n# ================================================================\nNUM_FOLDS = 5\nNUM_EPOCHS = 20          # you can go up to 30 if time allows\nTRAIN_BATCH_SIZE = 128\nLEARNING_RATE = 5e-4\n\nkfold_splitter = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=RANDOM_SEED)\n\noof_predictions = np.zeros(NUM_ORIGINAL, dtype=np.float32)\nfold_test_predictions = []\n\nidx_original_rows  = np.arange(NUM_ORIGINAL)\nidx_synthetic_rows = np.arange(NUM_ORIGINAL, NUM_TOTAL_ROWS)\n\nfor fold, (idx_train_original, idx_valid_original) in enumerate(kfold_splitter.split(idx_original_rows)):\n    print(f\"\\n===== Two-Tower NN Fold {fold+1}/{NUM_FOLDS} =====\")\n\n    # Train indices: original-train + ALL synthetic\n    idx_train_all = np.concatenate([idx_original_rows[idx_train_original], idx_synthetic_rows])\n    idx_valid_all = idx_original_rows[idx_valid_original]\n\n    X_train_batch = tensor_all_inputs[idx_train_all]\n    y_train_batch = tensor_all_targets[idx_train_all]\n    X_valid_batch = tensor_all_inputs[idx_valid_all]\n    y_valid_batch = tensor_all_targets[idx_valid_all]\n\n    train_data_loader = DataLoader(TensorDataset(X_train_batch, y_train_batch),\n                                   batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n    valid_data_loader = DataLoader(TensorDataset(X_valid_batch, y_valid_batch),\n                                   batch_size=TRAIN_BATCH_SIZE, shuffle=False)\n\n    model = DualTowerRegressor().to(COMPUTE_DEVICE)\n    model_optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n\n    best_val_rmse = 999\n    best_model_state = None\n\n    for epoch in range(1, NUM_EPOCHS + 1):\n        model.train()\n        total_loss = 0.0\n\n        for xb, yb in train_data_loader:\n            model_optimizer.zero_grad()\n            pred = model(xb)\n            loss = focal_weighted_mse(pred, yb, gamma=2.0)\n            loss.backward()\n            model_optimizer.step()\n            total_loss += loss.item()\n\n        avg_loss = total_loss / len(train_data_loader)\n\n        # validation\n        model.eval()\n        val_predictions = []\n        with torch.no_grad():\n            for xb, yb in valid_data_loader:\n                vp = model(xb).detach().cpu().numpy().ravel()\n                val_predictions.append(vp)\n        val_predictions = np.concatenate(val_predictions)\n        val_rmse = mean_squared_error(target_raw_train[idx_valid_all], val_predictions) ** 0.5\n\n        print(f\"Epoch {epoch:02d} | TrainLoss={avg_loss:.4f} | ValRMSE={val_rmse:.4f}\")\n\n        if val_rmse < best_val_rmse:\n            best_val_rmse = val_rmse\n            best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n\n    print(f\"Best ValRMSE Fold {fold+1}: {best_val_rmse:.4f}\")\n\n    # Load best state & compute OOF\n    model.load_state_dict({k: v.to(COMPUTE_DEVICE) for k, v in best_model_state.items()})\n    model.eval()\n\n    with torch.no_grad():\n        fold_val_preds = model(X_valid_batch).cpu().numpy().ravel()\n    oof_predictions[idx_valid_all] = fold_val_preds.astype(np.float32)\n\n    # Test predictions for this fold\n    with torch.no_grad():\n        test_predictions_fold = model(tensor_test_inputs).cpu().numpy().ravel()\n    fold_test_predictions.append(test_predictions_fold)\n\n    # cleanup\n    del model, model_optimizer, train_data_loader, valid_data_loader\n    torch.cuda.empty_cache()\n    gc.collect()\n\n# OOF RMSE on original (non-augmented) rows\noverall_oof_rmse = mean_squared_error(target_raw_train, oof_predictions) ** 0.5\nprint(\"\\n===== Two-Tower NN OOF RMSE (original data):\", overall_oof_rmse, \"=====\")\n\nmean_test_predictions = np.mean(fold_test_predictions, axis=0).astype(np.float32)\nprint(\"Test NN prediction range:\", mean_test_predictions.min(), \"to\", mean_test_predictions.max())\nfor a, b in [(0,1),(1,2),(2,3),(3,4),(4,5),(5,6),(6,7),(7,8),(8,9),(9,10)]:\n    c = ((mean_test_predictions >= a) & (mean_test_predictions < b)).sum()\n    print(f\"{a}-{b}: {c}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T16:00:32.604374Z","iopub.execute_input":"2025-11-19T16:00:32.604652Z","iopub.status.idle":"2025-11-19T16:01:07.960655Z","shell.execute_reply.started":"2025-11-19T16:00:32.604632Z","shell.execute_reply":"2025-11-19T16:01:07.959858Z"}},"outputs":[{"name":"stdout","text":"\n===== Two-Tower NN Fold 1/5 =====\nEpoch 01 | TrainLoss=506.0943 | ValRMSE=4.0326\nEpoch 02 | TrainLoss=275.7896 | ValRMSE=3.5054\nEpoch 03 | TrainLoss=240.0161 | ValRMSE=3.7378\nEpoch 04 | TrainLoss=214.1691 | ValRMSE=3.7100\nEpoch 05 | TrainLoss=200.9905 | ValRMSE=3.3091\nEpoch 06 | TrainLoss=188.4938 | ValRMSE=3.6571\nEpoch 07 | TrainLoss=182.1440 | ValRMSE=3.1690\nEpoch 08 | TrainLoss=176.9402 | ValRMSE=3.1029\nEpoch 09 | TrainLoss=167.0938 | ValRMSE=3.3162\nEpoch 10 | TrainLoss=166.6724 | ValRMSE=3.2912\nEpoch 11 | TrainLoss=159.4075 | ValRMSE=3.5842\nEpoch 12 | TrainLoss=159.0083 | ValRMSE=3.1013\nEpoch 13 | TrainLoss=152.4735 | ValRMSE=3.1006\nEpoch 14 | TrainLoss=148.0266 | ValRMSE=2.9738\nEpoch 15 | TrainLoss=150.4966 | ValRMSE=3.0113\nEpoch 16 | TrainLoss=146.3616 | ValRMSE=3.0637\nEpoch 17 | TrainLoss=140.8852 | ValRMSE=3.1213\nEpoch 18 | TrainLoss=143.2615 | ValRMSE=3.2054\nEpoch 19 | TrainLoss=146.2980 | ValRMSE=3.5214\nEpoch 20 | TrainLoss=138.9304 | ValRMSE=2.8980\nBest ValRMSE Fold 1: 2.8980\n\n===== Two-Tower NN Fold 2/5 =====\nEpoch 01 | TrainLoss=523.6621 | ValRMSE=4.0915\nEpoch 02 | TrainLoss=279.7991 | ValRMSE=3.5766\nEpoch 03 | TrainLoss=241.4322 | ValRMSE=3.6505\nEpoch 04 | TrainLoss=219.2103 | ValRMSE=3.6576\nEpoch 05 | TrainLoss=197.5303 | ValRMSE=3.0466\nEpoch 06 | TrainLoss=191.2419 | ValRMSE=3.6458\nEpoch 07 | TrainLoss=180.8344 | ValRMSE=3.4040\nEpoch 08 | TrainLoss=173.0716 | ValRMSE=3.1692\nEpoch 09 | TrainLoss=167.2430 | ValRMSE=3.1048\nEpoch 10 | TrainLoss=161.8493 | ValRMSE=2.6826\nEpoch 11 | TrainLoss=158.6248 | ValRMSE=3.1885\nEpoch 12 | TrainLoss=158.5607 | ValRMSE=3.0922\nEpoch 13 | TrainLoss=152.7006 | ValRMSE=2.9327\nEpoch 14 | TrainLoss=151.2706 | ValRMSE=3.5928\nEpoch 15 | TrainLoss=149.8124 | ValRMSE=3.4030\nEpoch 16 | TrainLoss=149.0488 | ValRMSE=3.1302\nEpoch 17 | TrainLoss=139.0941 | ValRMSE=2.7805\nEpoch 18 | TrainLoss=146.6633 | ValRMSE=3.3648\nEpoch 19 | TrainLoss=151.0613 | ValRMSE=3.2091\nEpoch 20 | TrainLoss=146.4747 | ValRMSE=3.1182\nBest ValRMSE Fold 2: 2.6826\n\n===== Two-Tower NN Fold 3/5 =====\nEpoch 01 | TrainLoss=506.1655 | ValRMSE=4.3059\nEpoch 02 | TrainLoss=276.5203 | ValRMSE=3.5503\nEpoch 03 | TrainLoss=229.0559 | ValRMSE=3.5695\nEpoch 04 | TrainLoss=221.6533 | ValRMSE=3.4183\nEpoch 05 | TrainLoss=207.6019 | ValRMSE=3.6821\nEpoch 06 | TrainLoss=189.3956 | ValRMSE=3.8537\nEpoch 07 | TrainLoss=180.1873 | ValRMSE=3.3209\nEpoch 08 | TrainLoss=169.5050 | ValRMSE=3.5589\nEpoch 09 | TrainLoss=170.2702 | ValRMSE=3.2417\nEpoch 10 | TrainLoss=166.4522 | ValRMSE=3.2188\nEpoch 11 | TrainLoss=154.0753 | ValRMSE=3.1775\nEpoch 12 | TrainLoss=145.7148 | ValRMSE=3.3972\nEpoch 13 | TrainLoss=152.2098 | ValRMSE=3.1681\nEpoch 14 | TrainLoss=147.3463 | ValRMSE=3.3883\nEpoch 15 | TrainLoss=152.0357 | ValRMSE=3.2249\nEpoch 16 | TrainLoss=138.8533 | ValRMSE=3.3268\nEpoch 17 | TrainLoss=138.6702 | ValRMSE=3.3088\nEpoch 18 | TrainLoss=142.6687 | ValRMSE=2.7911\nEpoch 19 | TrainLoss=138.0259 | ValRMSE=3.0391\nEpoch 20 | TrainLoss=141.3240 | ValRMSE=3.4608\nBest ValRMSE Fold 3: 2.7911\n\n===== Two-Tower NN Fold 4/5 =====\nEpoch 01 | TrainLoss=559.0322 | ValRMSE=3.9151\nEpoch 02 | TrainLoss=282.8956 | ValRMSE=3.6871\nEpoch 03 | TrainLoss=244.2310 | ValRMSE=3.6083\nEpoch 04 | TrainLoss=218.6063 | ValRMSE=3.3171\nEpoch 05 | TrainLoss=197.6191 | ValRMSE=3.4080\nEpoch 06 | TrainLoss=193.8126 | ValRMSE=3.6888\nEpoch 07 | TrainLoss=184.7168 | ValRMSE=3.6230\nEpoch 08 | TrainLoss=177.6386 | ValRMSE=3.3180\nEpoch 09 | TrainLoss=165.5915 | ValRMSE=3.2203\nEpoch 10 | TrainLoss=170.1637 | ValRMSE=3.2306\nEpoch 11 | TrainLoss=157.4562 | ValRMSE=3.4640\nEpoch 12 | TrainLoss=155.3040 | ValRMSE=2.9376\nEpoch 13 | TrainLoss=158.3723 | ValRMSE=3.2954\nEpoch 14 | TrainLoss=148.9788 | ValRMSE=2.9265\nEpoch 15 | TrainLoss=144.1658 | ValRMSE=3.0591\nEpoch 16 | TrainLoss=142.1304 | ValRMSE=3.1870\nEpoch 17 | TrainLoss=146.2637 | ValRMSE=2.8028\nEpoch 18 | TrainLoss=143.1772 | ValRMSE=3.1764\nEpoch 19 | TrainLoss=144.3308 | ValRMSE=3.0453\nEpoch 20 | TrainLoss=144.6509 | ValRMSE=3.0046\nBest ValRMSE Fold 4: 2.8028\n\n===== Two-Tower NN Fold 5/5 =====\nEpoch 01 | TrainLoss=537.3494 | ValRMSE=3.9708\nEpoch 02 | TrainLoss=273.8722 | ValRMSE=3.8518\nEpoch 03 | TrainLoss=237.6969 | ValRMSE=3.7954\nEpoch 04 | TrainLoss=212.4912 | ValRMSE=3.7849\nEpoch 05 | TrainLoss=198.3870 | ValRMSE=3.6678\nEpoch 06 | TrainLoss=193.5265 | ValRMSE=3.6297\nEpoch 07 | TrainLoss=177.7851 | ValRMSE=3.2521\nEpoch 08 | TrainLoss=172.2753 | ValRMSE=3.5038\nEpoch 09 | TrainLoss=170.5236 | ValRMSE=2.8912\nEpoch 10 | TrainLoss=168.4372 | ValRMSE=3.2408\nEpoch 11 | TrainLoss=158.3796 | ValRMSE=2.9139\nEpoch 12 | TrainLoss=156.7733 | ValRMSE=3.0434\nEpoch 13 | TrainLoss=153.9416 | ValRMSE=3.1142\nEpoch 14 | TrainLoss=148.0936 | ValRMSE=3.2720\nEpoch 15 | TrainLoss=147.7862 | ValRMSE=2.9665\nEpoch 16 | TrainLoss=146.0630 | ValRMSE=2.8778\nEpoch 17 | TrainLoss=146.2386 | ValRMSE=3.2612\nEpoch 18 | TrainLoss=140.8000 | ValRMSE=3.2378\nEpoch 19 | TrainLoss=141.2235 | ValRMSE=3.1205\nEpoch 20 | TrainLoss=136.2460 | ValRMSE=2.9545\nBest ValRMSE Fold 5: 2.8778\n\n===== Two-Tower NN OOF RMSE (original data): 2.811509784587161 =====\nTest NN prediction range: -1.6048301 to 12.388692\n0-1: 303\n1-2: 336\n2-3: 111\n3-4: 96\n4-5: 93\n5-6: 881\n6-7: 363\n7-8: 379\n8-9: 471\n9-10: 258\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Cell 8: Save out-of-fold/test predictions and create the submission file.\nnp.save(os.path.join(OUTPUT_PATH, \"two_tower_oof.npy\"),  oof_predictions)\nnp.save(os.path.join(OUTPUT_PATH, \"two_tower_test.npy\"), mean_test_predictions)\n\n# Continuous predictions clipped to [0,10] (better for RMSE than rounding)\nclipped_test_predictions = np.clip(mean_test_predictions, 0.0, 10.0)\n\n# If test has ID column use it, else 1..N\nid_column_name = \"ID\" if \"ID\" in df_test_main.columns else None\n\nif id_column_name:\n    submission_df = pd.DataFrame({\n        \"ID\": df_test_main[id_column_name].values,\n        \"score\": clipped_test_predictions\n    })\nelse:\n    submission_df = pd.DataFrame({\n        \"ID\": np.arange(1, len(clipped_test_predictions) + 1),\n        \"score\": clipped_test_predictions\n    })\n\nsubmission_path = os.path.join(OUTPUT_PATH, \"submission_22.csv\")\nsubmission_df.to_csv(submission_path, index=False)\nprint(\"Saved submission:\", submission_path) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T16:02:29.986946Z","iopub.execute_input":"2025-11-19T16:02:29.987760Z","iopub.status.idle":"2025-11-19T16:02:30.001712Z","shell.execute_reply.started":"2025-11-19T16:02:29.987733Z","shell.execute_reply":"2025-11-19T16:02:30.001096Z"}},"outputs":[{"name":"stdout","text":"Saved submission: /kaggle/working/submission_22.csv\n","output_type":"stream"}],"execution_count":12}]}